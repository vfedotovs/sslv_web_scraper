#!/usr/bin/env python3
"""
This is entry point for module that provides basic API interface which will trigger web scrape job for specific city.

This module contains functions:
- home
- async run_long_task
- check_today_cloud_data_file_exist
- get_todays_cloud_data_file_name
- check_lst_run_state

"""

from datetime import datetime
import logging
import logging.handlers as handlers
from logging.handlers import RotatingFileHandler
import os
import sys
import uvicorn
from fastapi import FastAPI
from app.wsmodules.file_downloader import download_latest_lambda_file
from app.wsmodules.web_scraper import scrape_website
from app.wsmodules.data_format_changer import cloud_data_formater_main
from app.wsmodules.df_cleaner import df_cleaner_main
from app.wsmodules.db_worker import db_worker_main
from app.wsmodules.analytics import analytics_main
from app.wsmodules.pdf_creator import pdf_creator_main
from app.wsmodules.sendgrid_mailer import sendgrid_mailer_main


log = logging.getLogger('fastapi')
log.setLevel(logging.INFO)
fastapi_log_format = logging.Formatter(
    "%(asctime)s [%(levelname)-5.5s] : %(funcName)s:"
    " %(lineno)d: %(message)s")

ch = logging.StreamHandler(sys.stdout)
ch.setFormatter(fastapi_log_format)
log.addHandler(ch)

fh = handlers.RotatingFileHandler('ws_main.log',
                                  maxBytes=(1048576*5),
                                  backupCount=5)
fh.setFormatter(fastapi_log_format)
log.addHandler(fh)

app = FastAPI()
CITY_NAME = 'Ogre'
# Load the feature flag from the environment
USE_AWS_LAMBDA_FILE = os.getenv("USE_AWS_LAMBDA_FILE", "False").lower() == "true"


@app.get("/")
def home():
    """Test enpoint to verify if fast-api is live"""
    log.info("Recieved GET request on / FastAPI server is ready ...")

    return {"FastAPI server is ready !!!"}


@app.get("/run-task/{city}")
async def run_long_task(city: str):
    """ Endpint to trigger scrape, format and insert data in DB"""
    log.info("Recieved GET request to start scraping job for %s city", city)
    if USE_AWS_LAMBDA_FILE:
        download_latest_lambda_file()
    else:
        log.warning("Skipping download as USE_AWS_LAMBDA_FILE is set to False")
    todays_cloud_data_file_exist = check_today_cloud_data_file_exist()

    if todays_cloud_data_file_exist is True:
        last_cloud_file_name = get_todays_cloud_data_file_name()
        log.info("Cloud scraper module data file: %s "
                 "found will be used in "
                 " data_formater_module ", last_cloud_file_name)
        log.info("Running cloud_data_formater_main task: using cloud ws file")
        cloud_data_formater_main()
        log.info("Running df_cleaner_main task: using cloud ws file")
        df_cleaner_main()
        log.info("Running db_worker_main task: using cloud ws file")
        db_worker_main()
        log.info("Running analytics_main task: using cloud ws file")
        analytics_main()
        log.info("Running pdf_creator task: using cloud ws file ")
        pdf_creator_main()
        log.info("Running sendgrid_mailer task: using cloud ws file")
        sendgrid_mailer_main()
        log.info("Completed /run-task/ogre using AWS lambda raw-data file")
        return {
            "message": "FAST_API: scrape Ogre city apartments"
                       " task using cloud ws file run completed"
        }

    lst_run_state = check_lst_run_state(CITY_NAME)
    if lst_run_state:
        log.info("EXIT: will not call ws_worker "
                 "module because task was run last 24H")
        return {"message": "Local scraper job already has run,"
                           " in last 24H will not run today again"}

    if todays_cloud_data_file_exist is False:
        log.info("Running scrape_website task will create local ws file")
        scrape_website()
        log.info("Running data_formater_main task: using locally scraped file")
        cloud_data_formater_main()
        log.info("Running df_cleaner_main task: using locally scraped file")
        df_cleaner_main()
        log.info("Running db_worker_main task: using locally scraped file")
        db_worker_main()
        log.info("Running analytics_main task: using locally scraped file")
        analytics_main()
        log.info("Running pdf_creator task: using locally scraped")
        pdf_creator_main()
        log.info("Running sendgrid_mailer task: using locally scraped file")
        sendgrid_mailer_main()
        log.info("sendgrid_mailer_main task completed")
        return {"message": "FAST_API: scrape Ogre city apartments "
                "task using local scrape job was completed"}


def check_today_cloud_data_file_exist() -> bool:
    """Checks if file Ogre-raw-data-report-YYYY-MM-DDTMM-HH-SS.txt
    which was generated by AWS lambda scraper and downloaded from S3 bucket
    exist in local_lambda_raw_scraped_datafolder with todays date"""
    cloud_file_folder = "local_lambda_raw_scraped_data"
    todays_date = datetime.today().strftime('%Y-%m-%d')
    log.info("Searching for cloud files with todays date: %s ", todays_date)
    if not os.path.exists(cloud_file_folder):
        log.error("Folder %s does not exist "
                  "creating empty folder", cloud_file_folder)
        os.makedirs(cloud_file_folder)
    for file_name in os.listdir(cloud_file_folder):
        if todays_date in file_name:
            log.info('File %s containing today'
                     ' date %s found, ', file_name, todays_date)
            return True
    log.info('File containing today date %s was not found,'
             ' will try to find local craper file', todays_date)
    return False


def get_todays_cloud_data_file_name() -> str:
    """Checks if file Ogre-raw-data-report-YYYY-MM-DDTMM-HH-SS.txt
    which was generated by AWS lambda scraper and
    downloaded from S3 bucket exist in local_lambda_raw_scraped_data
    folder with todays date and returns file name"""

    cloud_file_folder = "local_lambda_raw_scraped_data"
    todays_date = datetime.today().strftime('%Y-%m-%d')
    log.info("Searching for cloud scraper file "
             " with todays date: %s ", todays_date)
    if not os.path.exists(cloud_file_folder):
        log.error("Folder %s does not exist "
                  "creating empty folder ", cloud_file_folder)
        os.makedirs(cloud_file_folder)
    for file_name in os.listdir(cloud_file_folder):
        if todays_date in file_name:
            log.info("File %s containing todays date"
                     "%s found", file_name, todays_date)
            return file_name


def check_lst_run_state(city_name) -> bool:
    """
    Returns true if Ogre-raw-data-report-YYYY-MM-DD.txt
    with todays date exists.
    """
    todays_date = datetime.today().strftime('%Y-%m-%d')
    target_filename = city_name + '-raw-data-report-' + todays_date + '.txt'
    if not os.path.exists('data'):
        os.makedirs('data')
    for filename in os.listdir("data"):
        if filename == target_filename:
            log.info('File %s found, task for '
                     'Ogre has run today', target_filename)
            return True
    log.info('File %s was not found, running '
             ' scrape task for Ogre city', target_filename)
    return False


if __name__ == "__main__":
    uvicorn.run(app)
