#!/usr/bin/env python3
"""
data_format_changer.py module functionality is to convert scraped multiline
text data per advert entry in to singe line CSV format that later can be loaded
in to pandas dataftame.

Module functions:
[x] Reads scraped data from Ogre-raw-data-report-2022-12-03.txt file
    (resolves limitation that text files can not be read by padas)
[x] Changes format 12 lines per ad entry to 1 line per ad entry.
[x] Writes pandas_df.csv and creates copy data/pandas_df.csv_2022-12-03.csv
    (later used as input file in analitics.py module and csv file can be very
    easy inported as pandas DataFrame.)
[x] Allows to utilise daily AWS labmda scrape job output file that leads to
    big perfommance improvement by eliminating need to re-scrape data on each
    app deployment (This architectual steep does reduce email recieving time
    by 4-5 min avoiding re-scraping data multiple times per day)


# TODO: Implement these features
[ ] Some data like Serija,Majas tips and Kadastra numurs are not
    transfered to outut file.
[ ] Rename pandas_df.csv_2022-12-03.csv to better name for example to
    ogre_city_data_2022_12_03.csv


Module requires:
[x] If today is 2022-12-03 file data/Ogre-raw-data-report-2022-12-03.txt 
    must exist for module to run

Mudule creates:
[x] File pandas_df.csv and makes copy data/pandas_df.csv_2022-12-03.csv
"""
import os
import re
from datetime import datetime
import logging
import logging.handlers as handlers
from logging.handlers import RotatingFileHandler
import sys
import pandas as pd


log = logging.getLogger('data_format_changer')
log.setLevel(logging.INFO)
fastapi_log_format = logging.Formatter(
    "%(asctime)s [%(levelname)-5.5s] : %(funcName)s:"
    " %(lineno)d: %(message)s")

ch = logging.StreamHandler(sys.stdout)
ch.setFormatter(fastapi_log_format)
log.addHandler(ch)

fh = handlers.RotatingFileHandler('df_changer.log',
                                  maxBytes=(1048576*5),
                                  backupCount=5)
fh.setFormatter(fastapi_log_format)
log.addHandler(fh)


def get_local_ws_fp() -> str:
    """Returns local web_scraper module
    output file path with todays date"""
    todays_date = datetime.today().strftime('%Y-%m-%d')
    target_filename = 'Ogre-raw-data-report-' + todays_date + '.txt'
    return "data/" + target_filename


def get_cloud_ws_fp() -> str:
    """Returns cloud web_scraper module
    output file path with todays date"""
    todays_date = datetime.today().strftime('%Y-%m-%d')
    target_filename = 'Ogre-raw-data-report-' + todays_date + '.txt'
    return "local_lambda_raw_scraped_data/" + target_filename


def check_todays_cloud_data_file_exist() -> bool:
    """Checks if file Ogre-raw-data-report-YYYY-MM-DDTMM-HH-SS.txt
    which was generated by AWS lambda scraper and downloaded from S3 bucket
    exist in local_lambda_raw_scraped_datafolder with todays date"""
    cloud_file_folder = "local_lambda_raw_scraped_data"
    todays_date = datetime.today().strftime('%Y-%m-%d')
    log.info("Attempting to find raw-data file scraped by lambda with date: %s ", todays_date)
    if not os.path.exists(cloud_file_folder):
        log.error("Folder %s does not exist. Creating empty folder ",
                  cloud_file_folder)
        os.makedirs(cloud_file_folder)
    cwd = os.getcwd()
    for file_name in os.listdir(cloud_file_folder):
        if todays_date in file_name:
            log.info('File %s containing today'
                     ' date %s found, ', file_name, todays_date)
            return True
    log.info('File containing date %s not found', todays_date)
    return False


def get_detailed_file_path() -> str:
    """ Returns detailed file name that matches todays date
    local_lambda_raw_scraped_data/Ogre-raw-data-report-YYYY-MM-DDTMM-HH-SS.txt
    """
    cloud_file_folder = "local_lambda_raw_scraped_data"
    todays_date = datetime.today().strftime('%Y-%m-%d')
    for file_name in os.listdir(cloud_file_folder):
        if todays_date in file_name:
            return "local_lambda_raw_scraped_data/" + file_name


def cloud_data_formater_main() -> None:
    """Read raw data from Ogre-raw-data-report.txt and save to pandas_df.csv"""
    log.info(' --- Started data_format_changer module ---')
    todays_cloud_ws_fp = get_cloud_ws_fp()
    log.info("Lambda scraped raw-data file path: %s ", todays_cloud_ws_fp)
    todays_local_ws_fp = get_local_ws_fp()
    todays_cloud_ws_file_exist = check_todays_cloud_data_file_exist()
    if todays_cloud_ws_file_exist is True:
        log.info("Lambda scraped raw-data file exists: %s ",
                str(todays_cloud_ws_file_exist))
        log.info("Creating one-line report from lambda scraped raw-data file: %s",
                 todays_cloud_ws_fp)
        detailed_cws_fp = get_detailed_file_path()
        ogre_city_data_frame = create_oneline_report(detailed_cws_fp)
        ogre_city_data_frame.to_csv("pandas_df.csv")   #286 BUG gets triggered here
        create_file_copy()
    elif todays_cloud_ws_file_exist is False:
        log.warning("Lambda scraped raw-data file does not exist, failing back to local scraper source file")
        log.info("Converting to csv format from local scraped raw-data file: %s format", todays_local_ws_fp)
        ogre_city_data_frame = create_oneline_report(todays_local_ws_fp)
        if ogre_city_data_frame is not None:
            log.info("Saving csv format data to DataFrame file pandas_df.csv ")
            ogre_city_data_frame.to_csv("pandas_df.csv")
            log.info("Saving csv format data file pandas_df.csv completed with success")
            create_file_copy()
        if ogre_city_data_frame is None:
            log.error('ogre_city_data_frame is None')
            log.error("Saving csv format data file pandas_df.csv has failed")
    log.info(' --- Finished data_format_changer module --- ')


def get_file_path(city_name: str) -> str:
    """Builds file name based on date"""
    todays_date = datetime.today().strftime('%Y-%m-%d')
    if city_name is None:
        log.error('Error: city_name is None')
    if city_name is not None:
        target_filename = city_name + '-raw-data-report-' + todays_date + '.txt'
        full_file_path = "data/" + target_filename
        log.info("File path: %s", full_file_path )
        return full_file_path


def create_oneline_report(source_file: str) -> pd.DataFrame:
    """Changes text file format(12 lines per ad entry)
    to csv file format (1 line per ad entry)

    12 line data format for each scraped ad entry in Ogre-raw-data-report-2022-12-03.txt
    https://ss.lv/msg/lv/real-estate/flats/ogre-and-reg/ogre/fxobe.html
    Pilsēta, rajons:><b>Ogre un raj.
    Pilsēta/pagasts:><b>Ogre
    Iela:><b>Jaunatnes iela 4
    Istabas:>2
    Platība:>50 m²
    Stāvs:>3/9/lifts
    Sērija:>602.
    Mājas tips:>Paneļu
    Kadastra numurs:>74019004158
    Price:>57 000 € (1 140 €/m²)
    Date:>01.02.2022


    Data format of pandas_df_2022-12-03.csv is one line per ad
    ,URL,Room_count,Size_sq_m,Floor,Street,Price,Pub_date
    0,https://ss.lv/msg/lv/real-estate/flats/ogre-and-reg/ogre/fxobe.html,Istabas:>2,Platiba:>50 m²,Stavs:>3/9/lifts,Iela:><b>Jaunatnes iela 4,Price:>57 000 € (1 140 €/m²),Date:>01.02.2022


    Args:
        source_file: text file data/Ogre-raw-data-report-2022-12-03.txt
    Returns:
       df: pd.DataFrame object
    """
    urls = []
    room_counts = []
    room_sizes = []
    room_streets = []
    room_prices = []
    room_floors = []
    publish_dates = []
    log.info("Converting raw-text 12 lines per entry fromat into 1 line per entry csv file format ")
    log.info("Reading data from file : %s", source_file )
    try:
        with open(source_file, 'r', encoding='utf-8') as file_handle:
            while True:
                line = file_handle.readline()
                match_url = re.search("https", line)
                if match_url:
                    urls.append(line.rstrip('\n'))
                match_room_count = re.search("Istabas:", line)
                if match_room_count:
                    room_counts.append(line.rstrip('\n'))
                match_room_street_count = re.search("Iela:", line)
                if match_room_street_count:
                    room_streets.append(line.rstrip('\n'))
                match_room_price = re.search("Price:", line)
                if match_room_price:
                    room_prices.append(line.rstrip('\n'))
                match_pub_date = re.search("Date:", line)
                if match_pub_date:
                    publish_dates.append(line.rstrip('\n'))
                match_room_size = re.search("Platība:", line)
                if match_room_size:
                    tmp = line.rstrip('\n')
                    sizes = tmp.replace("Platība:", "Platiba:")
                    room_sizes.append(sizes)
                match_room_floor = re.search("Stāvs:", line)
                if match_room_floor:
                    tmp = line.rstrip('\n')
                    floors = tmp.replace("Stāvs:", "Stavs:")
                    room_floors.append(floors)
                if not line:
                    break
            lists = [urls, room_counts, room_sizes, room_floors, room_streets, room_prices, publish_dates]
            validate_list_lengths(lists)
            # TODO: add fix for inconsistent list lenght scenario

            log.info("Creating dict datastructure from scraped raw data list datastructures")
            mydict = {'URL': urls,
                      'Room_count': room_counts,
                      'Size_sq_m': room_sizes,
                      'Floor': room_floors,
                      'Street': room_streets,
                      'Price': room_prices,
                      'Pub_date': publish_dates}
            try:
                log.info("Attempting to create the DataFrame ")
                pandas_df = pd.DataFrame(mydict)
            except Exception as e:
              log.error(f"Failed to create DataFrame: {str(e)}")
            log.info("DataFrame format was created successfully. ")
            return pandas_df
            
    except FileNotFoundError:
        log.error("Source raw-data text file: %s does not exist", source_file)
    except Exception as e:
        log.error(
            "An error occurred while processing the file %s : %s ", source_file, str(e))


def validate_list_lengths(lists) -> None:
    """
    Validates that all provided lists have the same length.

    Args:
        lists (list of lists): A list containing the lists to be validated.

    Raises:
        ValueError: If any of the lists have different lengths, the error is logged and then raised.
    """
    log.info("Validating that all provided data element lists have the same length")
    list_lengths = [len(lst) for lst in lists]
    if len(set(list_lengths)) > 1:
        error_message = f"All lists must have the same length. Found lengths: {list_lengths}"
        log.error(error_message)
        raise ValueError(error_message)
    log.info("Validation for all provided data element lists completed successfully")


def create_file_copy() -> None:
    """Creates copy of pandas_df.csv in as data/pandas_df.csv_2022-12-03.csv"""
    # TODO: add more robust try except mv
    todays_date = datetime.today().strftime('%Y-%m-%d')
    dest_file = 'pandas_df_' + todays_date + '.csv'
    copy_cmd = 'cp pandas_df.csv data/' + dest_file
    log.info("Creating backup of file: %s in to folder data/ ", dest_file)
    if not os.path.exists('data'):
        log.warning("data folder does not exist creating folder")
        os.makedirs('data')
    os.system(copy_cmd)
    log.info("Completed file: %s move to folder data/ with success", dest_file)


if __name__ == "__main__":
    cloud_data_formater_main()
